version: "3.8"

services:
  app:
    build: .
    container_name: etl_data_pipeline
    depends_on:
      - db
      - airflow
    environment:
      - DATABASE_URL=${DATABASE_URL} # Using the variable from .env file
    volumes:
      - .:/app
    command: ["python", "main.py"]

  db:
    image: postgres:15
    container_name: postgres_db
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports:
      - "5432:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data

  airflow:
    image: apache/airflow:2.5.0
    container_name: airflow_scheduler
    restart: always
    environment:
      - AIRFLOW__CORE__EXECUTOR=${AIRFLOW_EXECUTOR} # Using the variable from .env file
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=${AIRFLOW_SQL_ALCHEMY_CONN} # Using the variable from .env file
    ports:
      - "8080:8080"
    depends_on:
      - db
    volumes:
      - airflow_data:/opt/airflow

volumes:
  pg_data:
  airflow_data:

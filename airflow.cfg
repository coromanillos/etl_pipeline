[core]
# Airflow home directory
airflow_home = /app

# Database Connection
sql_alchemy_conn = postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}

# Executor (Docker Compose -> LocalExecutor is fine for simplicity)
executor = LocalExecutor

# Fernet key for securing password fields
fernet_key = P-g3OjEatT9s930lJ_JeC_Nm8ZbLiktLq1NE99z2570=

# Base URL for Airflow Web UI
base_url = http://localhost:8080

# Loading of example DAGs
load_example_dags = False


[webserver]
# Web server settings
web_server_port = 8080
secret_key = CHANGE_THIS_TO_A_SECURE_RANDOM_STRING


[smtp]
# Email alerts configuration
smtp_host = smtp.gmail.com
smtp_port = 587
smtp_starttls = True
smtp_ssl = False
smtp_user = ${SMTP_USER}
smtp_password = ${SMTP_PASSWORD}
smtp_mail_from = ${SMTP_USER}

[logging]
# Use custom logging configuration
# This will reuse your airflowLocalSettings with custom handlers
logging_config_class = 
# File handlers disabled; everything goes to console (JSON)

# NOTE: Fluent Bit parses container stdout (JSON) and forwards it to S3


[scheduler]
# Run all scheduled DAGs
job_heartbeat_sec = 10
min_file_process_interval = 30


[api]
# Enable API
auth_backend = airflow.api.auth.backend.default


[debug]
# Optional debug
# Uncomment for debug messages
# logging_level = DEBUG


[cli]
# CLI settings
api_client = airflow.api.client.local


[plugins]
# Enable or disable plugin directory
plugins_folder = /app/plugins


[admin]
# Administrator settings
# typically not used much

# ETL Pipeline for Alpha Vantage Stock Data ğŸ“ˆ

A production-ready ETL pipeline that extracts, transforms, and loads stock market data from the Alpha Vantage API into a PostgreSQL database. The pipeline is fully containerized with Docker, orchestrated using Apache Airflow, and includes automated logging and testing.

---

## Table of Contents ğŸ“Œ

- [Overview](#overview)
- [Features](#features)
- [Architecture](#architecture)
- [Data Sources](#data-sources)
- [Tech Stack](#tech-stack)
- [Getting Started](#getting-started)
- [Running the Pipeline](#running-the-pipeline)
- [Running Tests](#running-tests)
- [Future Enhancements](#future-enhancements)
- [Contributors](#contributors)

---

## Overview ğŸ§ 

This project demonstrates a real-world ETL pipeline, built with best practices in mind for data engineering workflows. It includes:

- Data ingestion from multiple formats (JSON, CSV)
- Automated transformations and validation
- Cloud-ready loading strategies
- Modular, testable, and containerized codebase

---

## Features ğŸš€

- ğŸ”Œ Connects to multiple APIs and internal data files
- ğŸ”„ Handles API rate limits and retries
- ğŸ§¹ Cleans and normalizes raw financial data
- ğŸ“¦ Loads to a cloud data warehouse (PostgreSQL, Redshift, or Snowflake)
- ğŸ“… Automated weekly runs with Airflow or `cron`
- ğŸ” Full logging and auditing for each step
- ğŸ§ª Includes unit, integration, and end-to-end tests

---

## Architecture ğŸ—ï¸

### 1. Extraction
- Uses `requests` to extract API data with retry/backoff
- Reads internal CSV/JSON using `pandas`

### 2. Transformation
- Cleans and normalizes fields
- Converts timestamps to UTC
- Handles nulls, duplicates, and categoricals

### 3. Loading
- Uses `sqlalchemy` or `psycopg2` to load data into PostgreSQL or cloud warehouses
- Maintains audit logs for inserts

### 4. Scheduling & Orchestration
- Orchestrated with **Apache Airflow** or scheduled with `cron`
- Logging handled by `loguru` or Pythonâ€™s built-in `logging`

---

## Data Sources ğŸ“Š

### âœ… API: Transaction Records (CSV)
- **Endpoint:** `https://api.bank.com/transactions`
- **Fields:** `transaction_id`, `account_number`, `amount`, `timestamp`, `category`
- **Auth:** API Key

### âœ… API: Customer Info (JSON)
- **Endpoint:** `https://api.bank.com/customers`
- **Fields:** `customer_id`, `name`, `dob`, `address`, `phone_number`, `email`
- **Auth:** OAuth 2.0

### âœ… Internal CSV: Savings Account Data
- **Path:** `/data/savings_accounts.csv`

### âœ… Internal JSON: Customer Metadata
- **Path:** `/data/customers_metadata.json`

---

## Tech Stack ğŸ› ï¸

- **Language:** Python 3.11
- **Libraries:** `pandas`, `requests`, `sqlalchemy`, `psycopg2`, `loguru`
- **Containerization:** Docker, Docker Compose
- **Orchestration:** Apache Airflow, cron
- **Storage:** PostgreSQL, AWS S3, Redshift, Snowflake
- **Monitoring:** ELK Stack, Prometheus (optional)
- **Testing:** `pytest`, `Makefile`

---

## Getting Started âš™ï¸

```bash
# Clone the repo
git clone https://github.com/your-username/savings-bank-pipeline.git
cd savings-bank-pipeline

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.example .env
# Fill in API keys, database URIs, etc.

# Run manually (optional)
python run_pipeline.py

# config.yaml file

api:
  endpoint: https://www.alphavantage.co/query
  key: your_api_key
  timeout: 30 # in seconds
  symbol: IBM
  interval: 5min

validation:
  alpha_vantage_intraday:
    required_keys:
      - endpoint
      - key
      - timeout
      - symbol
      - interval

directories:
  raw_data: "../data/raw_data"
  processed_data: "../data/processed_data"
  dags: "../dags" # Add dags folder path for Airflow DAGs
  logs: "../logs" # Add logs folder for Airflow logs

logging:
  level: DEBUG # Log level
  format: json # Structured JSON logging for container aggregation
  # No log files needed here; logs routed to Airflow's logging system or stdout

extract:
  required_fields:
    - "1. open"
    - "2. high"
    - "3. low"
    - "4. close"
    - "5. volume"

transform:
  required_fields:
    - "1. open"
    - "2. high"
    - "3. low"
    - "4. close"
    - "5. volume"

postgres_loader:
  # Logging is centralized globally
  connection_string: "postgresql://cromanillos:werc3871874@db:5432/etl_project" # Add your DB connection string here
  schema: public
  table: your_target_table # Replace with your target table name

airflow:
  executor: LocalExecutor # Matches your docker-compose .env setting
  dags_folder: "../dags"
  logs_folder: "../logs"
